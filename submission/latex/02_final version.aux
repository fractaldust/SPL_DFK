\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{ecta}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\citation{manyika2011}
\citation{crone2006}
\citation{lessmann2015}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}{section.1}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces cost matrix, consisting of type one and type two errors and depending only on item price}}{3}{table.1}}
\newlabel{cost matrix}{{1}{3}{cost matrix, consisting of type one and type two errors and depending only on item price}{table.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Data}{4}{section.2}}
\newlabel{Sec::Data}{{2}{4}{Data}{section.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces first look on raw data set, containing 13 variables plus the \textit  {return} variable for the \textit  {known} data set. Abbreviations: int - integer, chr -character.}}{4}{table.2}}
\newlabel{raw data set}{{2}{4}{first look on raw data set, containing 13 variables plus the \textit {return} variable for the \textit {known} data set. Abbreviations: int - integer, chr -character}{table.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Feature Engineering}{5}{section.3}}
\newlabel{Sec::Feature Eng}{{3}{5}{Feature Engineering}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Definition and explanation of variables }{5}{subsection.3.1}}
\newlabel{Subsec::variables}{{3.1}{5}{Definition and explanation of variables}{subsection.3.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Overview of the main features that are used for the predictive model.}}{5}{table.3}}
\newlabel{Table::Features}{{3}{5}{Overview of the main features that are used for the predictive model}{table.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}The case of \texttt  {item\_retrate}}{6}{subsection.3.2}}
\newlabel{Subsec::ItemRetrate}{{3.2}{6}{The case of \texttt {item\_retrate}}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Interval boundary choice}{7}{subsubsection.3.2.1}}
\newlabel{Subsec::Interval}{{3.2.1}{7}{Interval boundary choice}{subsubsection.3.2.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Classification outcomes for $N_{item}=6$ (left) and $N_{item}=10$ (right).}}{8}{table.4}}
\newlabel{Table::probs}{{4}{8}{Classification outcomes for $N_{item}=6$ (left) and $N_{item}=10$ (right)}{table.4}{}}
\citation{moeyersoms2015}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Definition of tau categories. Every order is sorted to an tau category, depending on the price of the item that got ordered.}}{9}{table.5}}
\newlabel{Table::intervals}{{5}{9}{Definition of tau categories. Every order is sorted to an tau category, depending on the price of the item that got ordered}{table.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Weight of Evidence}{9}{subsection.3.3}}
\newlabel{Subsec::WOE}{{3.3}{9}{Weight of Evidence}{subsection.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Weight of Evidence (WOE) of the feature \texttt  {item\_retrate}. Categories "very low" to "very high" were created by grouping the items according to their return rate in the training set. This created a strong feature in terms of WOE.}}{9}{figure.1}}
\newlabel{Figure::ItemRetrateWOE}{{1}{9}{Weight of Evidence (WOE) of the feature \texttt {item\_retrate}. Categories "very low" to "very high" were created by grouping the items according to their return rate in the training set. This created a strong feature in terms of WOE}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces (a) Display of the artificially created feature \texttt  {item\_retrate}. red shading: average return rate of observations of this category with comparison to average return rate of all items in \texttt  {train} set (red line, avg.returnrate = 0.48174). blue histogram: number of observations in each category. (b) Distribution of variable \texttt  {item\_retrate} in \texttt  {train} and \texttt  {test} set. In both data sets the newly created variable follows a similar distribution.}}{10}{figure.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{10}{figure.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{10}{figure.2}}
\newlabel{Figure::ItemRetrate}{{2}{10}{(a) Display of the artificially created feature \texttt {item\_retrate}. red shading: average return rate of observations of this category with comparison to average return rate of all items in \texttt {train} set (red line, avg.returnrate = 0.48174). blue histogram: number of observations in each category. (b) Distribution of variable \texttt {item\_retrate} in \texttt {train} and \texttt {test} set. In both data sets the newly created variable follows a similar distribution}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}semantic grouping: the case of \texttt  {item\_type}}{10}{subsection.3.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Display of the artificially created feature \texttt  {item\_type}. red shading: average return rate of orders in this categories in training set. blue histogram: number of orders of each category.}}{11}{figure.3}}
\newlabel{Figure::ItemType}{{3}{11}{Display of the artificially created feature \texttt {item\_type}. red shading: average return rate of orders in this categories in training set. blue histogram: number of orders of each category}{figure.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Preparation of Data Set for Models}{11}{section.4}}
\newlabel{Sec::PrepDataSet}{{4}{11}{Preparation of Data Set for Models}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Decomposition due to uncertain categories}{11}{subsection.4.1}}
\newlabel{Subsec::4Split}{{4.1}{11}{Decomposition due to uncertain categories}{subsection.4.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Decomposition of data set due to uncertain categories. u/n stands for \texttt  {unknown} and \texttt  {new} categories (= uncertain categories), 1 for certain categories. Categories \texttt  {unknown} and \texttt  {new} in \texttt  {item\_retrate} and \texttt  {user\_retrate} are uncertain and should not be included in models. Decomposition in pure subsets ensures that model only is fed with certain categories.}}{12}{table.6}}
\newlabel{Figure::Decomposition}{{6}{12}{Decomposition of data set due to uncertain categories. u/n stands for \texttt {unknown} and \texttt {new} categories (= uncertain categories), 1 for certain categories. Categories \texttt {unknown} and \texttt {new} in \texttt {item\_retrate} and \texttt {user\_retrate} are uncertain and should not be included in models. Decomposition in pure subsets ensures that model only is fed with certain categories}{table.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}On multiple $\tau $ thresholds}{13}{subsection.4.2}}
\newlabel{Subsec::Tau}{{4.2}{13}{On multiple $\tau $ thresholds}{subsection.4.2}{}}
\newlabel{boobies}{{1}{13}{On multiple $\tau $ thresholds}{equation.4.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Definition of tau categories. Every order is sorted to an tau category, depending on the price of the item that got ordered.}}{13}{table.7}}
\newlabel{Table::Tau}{{7}{13}{Definition of tau categories. Every order is sorted to an tau category, depending on the price of the item that got ordered}{table.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Cross Validation}{14}{section.5}}
\newlabel{Sec::CV}{{5}{14}{Cross Validation}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Theory}{14}{subsection.5.1}}
\citation{tibshirani2009}
\newlabel{Sub::stratified}{{5.1}{15}{N-repeated stratified K-fold C.V}{section*.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Implementation}{15}{subsection.5.2}}
\citation{chen2016}
\@writefile{toc}{\contentsline {section}{\numberline {6}Classification Alogrithms}{19}{section.6}}
\newlabel{Sec::Algorit}{{6}{19}{Classification Alogrithms}{section.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Extreme Gradient Boosting}{19}{subsection.6.1}}
\citation{breiman2001random}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Random Forest}{20}{subsection.6.2}}
\citation{breiman2001random}
\citation{lecun2015}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces An example of a neural network structure map. In this particular case there are three input variables, two hidden layers consisting of four and three neurons respectively and two output variables. Note that in the \textit  {nnet} R-package there is only one hidden layer. Source: \cite  {lecun2015}}}{23}{figure.4}}
\newlabel{Figure::Nnet_1}{{4}{23}{An example of a neural network structure map. In this particular case there are three input variables, two hidden layers consisting of four and three neurons respectively and two output variables. Note that in the \textit {nnet} R-package there is only one hidden layer. Source: \cite {lecun2015}}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Neural Network}{23}{subsection.6.3}}
\citation{lecun2015}
\citation{lecun2015}
\citation{moeyersoms2015}
\citation{moeyersoms2015}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.1}Backpropagation}{24}{subsubsection.6.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.2}Additional Data Preparation}{24}{subsubsection.6.3.2}}
\newlabel{Subsec::AddPrep}{{6.3.2}{24}{Additional Data Preparation}{subsubsection.6.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Diagram on how the backpropagation procedure is carried out. Starting from the error $y_{l}-t_{l}$, the chain rule of derivatives is used to explicit a variation in the error as function of variations in the input variables. Note that the error is taken as the differentiation of a loss function: in this case $\frac  {1}{2} (y_{l}-t_{l})^{2}$. Source: \cite  {lecun2015}}}{25}{figure.5}}
\newlabel{Figure::Nnet_2}{{5}{25}{Diagram on how the backpropagation procedure is carried out. Starting from the error $y_{l}-t_{l}$, the chain rule of derivatives is used to explicit a variation in the error as function of variations in the input variables. Note that the error is taken as the differentiation of a loss function: in this case $\frac {1}{2} (y_{l}-t_{l})^{2}$. Source: \cite {lecun2015}}{figure.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Parameter Tuning}{25}{section.7}}
\newlabel{Sec::ParTun}{{7}{25}{Parameter Tuning}{section.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Parameters of Neural Network}{25}{subsection.7.1}}
\newlabel{Subsec::ParsNNet}{{7.1}{25}{Parameters of Neural Network}{subsection.7.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.1}optimal $\tau $ for loss function}{26}{subsubsection.7.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Tuning graph for the six different subsets (according to tau categories). For every subset 35 different models were trained and the loss of each prediction calculated. The settings change as follows: size = 3, 5, ... , 15 then dacay = 0.01, 0.1, 0.5, 0.8, 1.0. Stable loss values for bigger decays, size does not have much influence for high decay values.}}{27}{figure.6}}
\newlabel{Figure::ParameterGraph}{{6}{27}{Tuning graph for the six different subsets (according to tau categories). For every subset 35 different models were trained and the loss of each prediction calculated. The settings change as follows: size = 3, 5, ... , 15 then dacay = 0.01, 0.1, 0.5, 0.8, 1.0. Stable loss values for bigger decays, size does not have much influence for high decay values}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Sum of normed tuning graphs of the six subsets from graph \ref  {Figure::ParameterGraph}. Loss is the highest for bigger decay values. Size only has little influence on loss values for high decay values.}}{27}{figure.7}}
\newlabel{Figure::NormedSum}{{7}{27}{Sum of normed tuning graphs of the six subsets from graph \ref {Figure::ParameterGraph}. Loss is the highest for bigger decay values. Size only has little influence on loss values for high decay values}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Loss function for the six tau-categories. In each category the loss is calculated for each tau-candidate. The negative loss shows its maximum in the range between $\tau =0.45$ and $\tau = 0.65$.}}{28}{figure.8}}
\newlabel{Figure::FindTau}{{8}{28}{Loss function for the six tau-categories. In each category the loss is calculated for each tau-candidate. The negative loss shows its maximum in the range between $\tau =0.45$ and $\tau = 0.65$}{figure.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Performance Benchmark}{28}{subsection.7.2}}
\newlabel{my-label}{{7.2}{28}{Performance Benchmark}{subsection.7.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Estimated total loss of random forest, xgboost and nnet in comparison to trivial predictions. Performed on 25000 observations (25\% of train set.}}{28}{table.8}}
\citation{yu2006}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Prediction}{29}{subsection.7.3}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusion}{29}{section.8}}
\newlabel{Sec:Conc}{{8}{29}{Conclusion}{section.8}{}}
\citation{*}
\bibdata{lit}
\bibcite{breiman2001random}{{1}{2001}{{Breiman}}{{Breiman}}}
\bibcite{chen2016}{{2}{2016}{{Chen and Guestrin}}{{Chen and Guestrin}}}
\bibcite{crone2006}{{3}{2006}{{Crone et~al.}}{{Crone, Lessmann, and Stahlbock}}}
\bibcite{lecun2015}{{4}{2015}{{LeCun et~al.}}{{LeCun, Bengio, and Hinton}}}
\bibcite{lessmann2015}{{5}{2015}{{Lessmann et~al.}}{{Lessmann, Baesens, Seow, and Thomas}}}
\bibcite{manyika2011}{{6}{2011}{{Manyika et~al.}}{{Manyika, Chui, Brown, Bughin, Dobbs, Roxburgh, and Byers}}}
\bibcite{moeyersoms2015}{{7}{2015}{{Moeyersoms and Martens}}{{Moeyersoms and Martens}}}
\bibcite{tibshirani2009}{{8}{2009}{{Tibshirani and Tibshirani}}{{Tibshirani and Tibshirani}}}
\bibcite{yu2006}{{9}{2006}{{Yu et~al.}}{{Yu, Lai, Wang, and Huang}}}
