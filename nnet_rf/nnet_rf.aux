\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{ecta}
\citation{nature}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces An example of neural network structure map. In this particular case there are three input variables, two hidden layers consisting of four and three neurons respectively and two output variables. Note that in standard nnet package there is only one hidden layer. Source \cite  {nature}}}{1}{figure.1}}
\newlabel{Figure::Nnet_1}{{1}{1}{An example of neural network structure map. In this particular case there are three input variables, two hidden layers consisting of four and three neurons respectively and two output variables. Note that in standard nnet package there is only one hidden layer. Source \cite {nature}}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Nnet}{1}{section.1}}
\citation{nature}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Diagram on how the backpropagation procedure is carried out. Starting from the error $y_{l}-t_{l}$, the chain rule of derivatives is used to explicit a variation in the error as function of variations in the input variables. Note that the error is taken as the differentiation of a cost function: in this case $\frac  {1}{2} (y_{l}-t_{l})^{2}$}}{2}{figure.2}}
\newlabel{Figure::Nnet_2}{{2}{2}{Diagram on how the backpropagation procedure is carried out. Starting from the error $y_{l}-t_{l}$, the chain rule of derivatives is used to explicit a variation in the error as function of variations in the input variables. Note that the error is taken as the differentiation of a cost function: in this case $\frac {1}{2} (y_{l}-t_{l})^{2}$}{figure.2}{}}
\citation{breiman2001random}
\@writefile{toc}{\contentsline {section}{\numberline {2}Random Forest}{3}{section.2}}
